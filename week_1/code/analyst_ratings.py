# -*- coding: utf-8 -*-
"""analyst_ratings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fZAaIaZzdYblw4GNaoi2LrUMTOo2N-Xy
"""

import pandas as pd
let_me_cook=pd.read_csv("/content/raw_analyst_ratings.csv")
let_me_cook.head()

let_me_cook['head_line_len']=len(let_me_cook['headline'])

let_me_cook.head()

head_len_mean=let_me_cook['head_line_len'].mean()

head_len_mean

head_len_median=let_me_cook['head_line_len'].median()

head_len_median

head_len_std=let_me_cook['head_line_len'].std()

head_len_std

let_me_cook["publisher"].value_counts()

#from the above who wrote the most headlines
let_me_cook.groupby("publisher")["head_line_len"].mean()

let_me_cook.groupby("publisher").head()

publisher_per_article=let_me_cook.groupby("publisher")['headline'].count()

#here is the most active publishers
sorted_publisher_with_article=publisher_per_article.sort_values(ascending=False)
sorted_publisher_with_article

let_me_cook["publication_date"]=pd.to_datetime(let_me_cook["date"],errors='coerce')

let_me_cook.head()

let_me_cook["year"]=let_me_cook["publication_date"].dt.year

let_me_cook["month"]=let_me_cook["publication_date"].dt.month
let_me_cook['day'] =let_me_cook["publication_date"].dt.day
let_me_cook['weekly']=let_me_cook["publication_date"].dt.day_name()
let_me_cook.head()

daily_counts = let_me_cook.groupby('publication_date').size().reset_index(name='article_count')
daily_counts

let_me_cook.head()

let_me_cook['stock'].value_counts()

for i in let_me_cook.columns:
  print(i)

over_all_counts = let_me_cook.groupby('publication_date').size()
over_all_counts.plot(title='over all Publication Trends')

over_all_counts

#publication per year
yearly_over_all_counts = let_me_cook.groupby('year').size()
yearly_over_all_counts
yearly_over_all_counts.plot(title='over all Publication Trends')

#publication months in each year deeply
yearly_over_all_counts = let_me_cook.groupby(['year', 'month']).size()
yearly_over_all_counts
yearly_over_all_counts.plot(title='monthly all Publication Trends')

#publication months in each year weekly
yearly_over_all_counts = let_me_cook.groupby(['year', 'month', 'weekly']).size()
yearly_over_all_counts

yearly_over_all_counts.plot(title='weekly all Publication Trends')

#sentiment analysis
from textblob import TextBlob
def show_sentiment(text):
    blob= TextBlob(text)
    sentiment_score = blob.sentiment.polarity
    if sentiment_score > 0:
        return 'positive'
    elif sentiment_score < 0:
        return 'negative'
    else:
        return 'neutral'
let_me_cook['sentiment'] = let_me_cook['headline'].apply(show_sentiment)
let_me_cook.head()

setminet='positive'
setminet2='negative'
setminet3='neutral'
let_me_cook['sentiment'].str.contains(setminet).sum()

let_me_cook['sentiment'].str.contains(setminet2).sum()

let_me_cook['sentiment'].str.contains(setminet3).sum()

let_me_cook.info()

let_me_cook.shape

from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import spacy
load_spacy_english_model=spacy.load('en_core_web_sm')

#let remove stop words using funciton like is was has have
def text_preprocess(text):
  data=load_spacy_english_model(text)
  return [token.lemma_.lower() for token in data if not token.is_stop and not token.is_punct]
let_me_cook['preprocessed_headline'] = let_me_cook['headline'].apply(text_preprocess)
let_me_cook.head()

non_stop='price'
let_me_cook['preprocessed_headline'].str.contains(non_stop).sum()

all_words = [word for sublist in let_me_cook['preprocessed_headline'] for word in sublist]

word_freq = Counter(all_words)

for word, freq in word_freq.most_common(10):
    print(f"{word}: {freq}")

def extract_entities(text):
    doc = load_spacy_english_model(text)
    return [ent.text for ent in doc.ents]
let_me_cook['entities'] = let_me_cook['headline'].apply(extract_entities)
all_entities = [entity for sublist in let_me_cook['entities'] for entity in sublist]
entity_freq = Counter(all_entities)
for entity, freq in entity_freq.most_common(10):
    print(f"{entity}: {freq}")

for i in let_me_cook.columns:
  print(i)

import matplotlib.pyplot as plt
let_me_cook.set_index('publication_date', inplace=True)
let_me_cook.head()
daily_counts = let_me_cook.resample('D').size()
monthly_counts = let_me_cook.resample('M').size()
yearly_counts = let_me_cook.resample('Y').size()
plt.figure(figsize=(14, 7))
plt.subplot(3, 1, 1)
daily_counts.plot()
plt.title('Daily Publication Counts')
plt.xlabel('Date')
plt.ylabel('Number of Publications')

plt.subplot(3, 1, 2)
monthly_counts.plot()
plt.title('Monthly Publication Counts')
plt.xlabel('Date')
plt.ylabel('Number of Publications')

plt.subplot(3, 1, 3)
yearly_counts.plot()
plt.title('Yearly Publication Counts')
plt.xlabel('Date')
plt.ylabel('Number of Publications')

apple=

let_me_cook['publisher'].value_counts()
top_n = 10
top_publishers = publisher_counts.head(top_n)
import seaborn as sns

plt.figure(figsize=(12, 8))
sns.barplot(x=top_publishers.index, y=top_publishers.values, palette='viridis')
plt.title(f'Top {top_n} Publishers by Number of Articles')
plt.xlabel('Publisher')
plt.ylabel('Number of Articles')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 8))
top_publishers.plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('viridis', top_n))
plt.title('Distribution of Articles Among Top Publishers')
plt.ylabel('')
plt.show()

def extract_domain_names(email):
  return email.split('@')[-1]

let_me_cook['domain'] = let_me_cook['publisher'].apply(extract_domain_names)

let_me_cook.head()

# #TASK 2

# apple=pd.read_csv("/content/AAPL_historical_data.csv")
# amazon=pd.read_csv("/content/AMZN_historical_data.csv")
# google=pd.read_csv("/content/GOOG_historical_data.csv")
# microsoft=pd.read_csv("/content/MSFT_historical_data.csv")
# meta=pd.read_csv("/content/META_historical_data.csv")
# nevida=pd.read_csv("/content/NVDA_historical_data.csv")
# tesla=pd.read_csv("/content/TSLA_historical_data.csv")


